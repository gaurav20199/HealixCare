data.sql
data.sql will run everytime on application startup. Although it is useful for development but it will lead to duplicate
entries better way is to add sample data in a migration script and activate the script only for dev environment.

Validation Groups
This is a concept where we provide constraints that will help us to recognize which specific fields need to be validated
depending upon whether it is a post request or a put request.

@Validated-> Using this annotation, we can use validation groups.
Complex scenarios, where you want different validation logic for different operations (create, update, partial update,
etc)— you can mark certain fields mandatory only in some cases.

What does Docker solves??
In Prod environment, we might be using alot of tools like db, redis, proxy. One way is to install all these tools one by
one either manually or through scripts which is tedious and time consuming.
Other issue is that we might end up installing other versions of these tools and there will be difference in prod and
dev environment.
Docker helps in containerizing all the tools, configs, code and other necessary dependencies that we require within a
docker environment.
So Docker makes sure, if the docker container works in mac/windows then the same docker container will also work in any
cloud like GCP.

How does Docker work??
1) In our project we usually have our code, runtime(that support program's execution), configs and dependencies.
We can bundle all these into a Docker image

Docker Image
A Docker image is a standalone, executable package that includes everything needed to run an application: your app’s
code, runtime, system tools, libraries, and settings.
It is like a blueprint or template for creating containers.

2) Once Docker image is created we can push it to Docker image repository(cloud or local). This is where Docker images
are stored, managed and Distributed. Eg: DockerHub

3) Using the Docker image repository, we can pull the image anywhere and can use the image to start our docker container
and this container can run anywhere docker is installed.


Error faced during Docker setup
1) Flyway takes initial connection and got timezone issue. Initial value was going as Asia/Calcutta to Flyway from some
reason. On setting the timezone from JVM Arguments, the issue was resolved.
2) If application.properties doesn't provide server.port then it will be taken as 8080. App will start on this port
inside docker container. If you will provide wrong binding like 8000:8000 then host will take the request on 8080 port
docker will accept the request on 8000 port. it will try to see if any process is running on 8000 port inside container
It won't find it as our spring boot app is running on 8080 port and it will cause issues.


Kafka usecase in the project
Suppose whenever patient gets created, we might need other services to Know about it. Those services can be for eg:
analytics service, notification service(that might trigger an email to verify email address) and other services.
Now one way is to process it synchronously like once patient is saved, make a call to analytics service and then to
notification service, just like we did a synchronous call to Billing Service.
Problem with this approach is that we have to make calls to multiple services and this causes things to slow down as
in real world scenario, every second there can be thousands of new patient created and for each patient we are calling
multiple services and hence there be thousands of service to service internal calls and even more per second which can
cause things to slow down tremendously.
Other drawback is that since all of this is blocking call so if suppose analytics service is slow or not available
it can bottleneck the entire system.

Solution: Asynchronous Communication
Patient Service can publish an event to Kafka and then all the services interested in that event can consume it.
This way Patient Service is not waiting for other services for response. Other services can consume the event as per
their convenience.

Kafka Terminologies
1) Kafka Broker: A single Kafka server is called a Kafka Broker. That Kafka broker is a program that runs on the Java
Virtual Machine (Java version 11+ for latest kafka version) and usually a server that is meant to be a Kafka broker will
solely run the necessary program and nothing else.
There can be multiple Kafka Brokers so that if one Broker is down whole system doesn't go down.
Brokers are identified by unique IDs (e.g., broker 0, broker 1, broker 2).

2) Kafka Cluster: A kafka cluster consists of multiple Kafka Brokers each running on different servers or containers.

3) Controller Broker: The controller broker is responsible for managing and administrative operations. It is important
for cluster coordination. It is responsible for operations like:
3.1) Creating/deleting topics
3.2) Adding partitions
3.3) Assigning leaders for partitions
3.4) Monitoring broker failures
3.5) Coordinating partition leader elections

4) Kafka Topic: A Kafka topic is a named category within a Kafka cluster where records (events/messages) are stored.
Producers write messages to topics, and consumers read messages from topics. There can be multiple Kafka Topics within
a Kafka Broker

5) Partitions: Each topic is internally divided into partitions for scalability and parallelism.

6) Kafka Event: These are stored inside Kafka Topic. This can be in JSON/Protobuf or custom format as required.

7) Internal and External Ports
7.1) External Ports (Client Communication)
These ports are used for communication between Kafka clients (producers, consumers, etc.) and the Kafka brokers
(the servers that run Kafka).
7.2) 


Setting up Kafka
1) Kafka Broker-> We can easily setup using Docker. Get the Docker image for Kafka


Checking environment variables inside docker
docker exec -it patient-service sh -c "printenv"

Pagination in Spring Data JPA
1) It works on 0 based indexing. In real life, page will start from 1. So Request and Response Handling is required.
2) If sort is applied then sort will take precedence i.e first the records from db will be sorted and then
offset and limit(page and size) will be applied on the sorted results.

Caching Data through Redis
Why we are caching Get API(s)
1) Get API(s) have to read alot of data from the database. To avoid frequent db queries, we can cache it.
2) The Get API(s) will mostly have static data(close to static data) so we should be okay with caching that data
with appropriate cache expiry time.

Redis will store the data in Key and Value format
What should be the Key??
Key should be unique so we can use url params to form a unique key and use it for saving data.
For eg: patients::page=1&size=5&sort=desc
Value in our case will be json as we are already using json to share patient response.

Rate Limiting
This helps in restricting user/client to only make a certain number of calls to a service within a time period.
This can be applied on basis of IP Address, user, API Key or Route etc.
This prevents system and resources from being overwhelmed and prevents resources abuse.

Circuit Breaker
Protects the system from cascading failures by cutting off calls to failing services.
Avoids wasting resources on repeated failed calls and allows the system some time to recover gracefully.
With a fallback mechanism, retries and timeout strategies we can implement circuit breaker.

In our project, patient service after creation of patient account, creates a billing account. If billing service
is down then it can cause issue in patient service too hence we can apply circuit breaker concept when we call
our billing service api(s).
Fallback should have similar method signature and should have similar Response as original method response. This
helps the client(who is calling the billing service) to not know whether fallback method is triggered or the original
method has been triggered so client don't have to do any special handling for fallback method.

Observability
It helps us understand what's happening inside our system through metrics, logs and traces.
Spring Boot Actuator will expose build in prod ready endpoints for health checks and metrics.
Prometheus collects this metrics and store it as time series metrics, giving real time insight.
Grafana provides dashboards and visualization using the raw metrics collected through Prometheus.

CQRS(Command Query Read Segregation)
Pattern implies that we should split our reads(query) and writes(commands) in separate models.
How it will be used in our project??
We have appointment service and patient service. Appointment service would also have to store the patient info as it
requires necessary info about the patient.
Ownership of Patient entity will lie with Patient Service but there will be a copy of patient entity inside appointment
service as well. Copy will be used for reading the data.

How data will be made consistent for Patient entity between the two service??
Whenever there will be any change in patient data, event will be shared through Kafka for it..Appointment service will
be subscribing for patient topic and will do the update according to that in their db.